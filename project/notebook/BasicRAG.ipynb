{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uv\n",
      "  Downloading uv-0.9.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading uv-0.9.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.2/21.2 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uv\n",
      "Successfully installed uv-0.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `--system` flag has no effect, a system Python interpreter is always used in `uv venv`\u001b[0m\n",
      "Using CPython 3.12.11 interpreter at: \u001b[36m/usr/bin/python3\u001b[39m\n",
      "Creating virtual environment at: \u001b[36mmy_colab_env\u001b[39m\n",
      "Activate with: \u001b[32msource my_colab_env/bin/activate\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!uv venv my_colab_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source my_colab_env/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m85 packages\u001b[0m \u001b[2min 1.99s\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m8 packages\u001b[0m \u001b[2min 1.01s\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m8 packages\u001b[0m \u001b[2min 61ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdataclasses-json\u001b[0m\u001b[2m==0.6.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfaiss-cpu\u001b[0m\u001b[2m==1.12.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==0.3.31\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarshmallow\u001b[0m\u001b[2m==3.26.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpypdf\u001b[0m\u001b[2m==6.1.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspect\u001b[0m\u001b[2m==0.9.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install langchain-community langchain faiss-cpu sentence-transformers pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m3 packages\u001b[0m \u001b[2min 86ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 26ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcolab-xterm\u001b[0m\u001b[2m==0.2.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install colab-xterm\n",
    "%load_ext colabxterm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain-community\n",
    "#!pip install langchain faiss-cpu sentence-transformers\n",
    "#!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!pip install colab-xterm #https://pypi.org/project/colab-xterm/\n",
    "%load_ext colabxterm\n",
    "\n",
    "!pip install colab-xterm -qqq\n",
    "!pip install langchain -qqq\n",
    "!pip install langchain_community -qqq\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m48 packages\u001b[0m \u001b[2min 215ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 10ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mollama\u001b[0m\u001b[2m==0.6.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install tensorflow psutil Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Info: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Total RAM: 12.671257019042969 GB\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from psutil import virtual_memory\n",
    "\n",
    "# Check GPU\n",
    "gpu_info = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU Info: {gpu_info}\")\n",
    "\n",
    "# Check RAM\n",
    "ram_info = virtual_memory()\n",
    "print(f\"Total RAM: {ram_info.total / (1024**3)} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k55t1BHHT9Fl"
   },
   "source": [
    "Run this in terminalto install ollama:\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "Run this in terminal to pull the desired model:\n",
    "ollama serve & ollama pull llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching Xterm..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(10000, {'cache': true}));\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%xterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ollama', 'pull', 'llama3'], returncode=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start Ollama server in the background\n",
    "ollama_server = subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "# Wait a few seconds to let the server start\n",
    "time.sleep(5)\n",
    "\n",
    "# Pull the LLaMA 3 model\n",
    "subprocess.run([\"ollama\", \"pull\", \"llama3\"], check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-299602144.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm=Ollama(model='llama3')\n"
     ]
    }
   ],
   "source": [
    "llm=Ollama(model='llama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1215690105.py:2: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response=llm.predict(question)\n"
     ]
    }
   ],
   "source": [
    "question=\"when did India get Independence?\"\n",
    "response=llm.predict(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India gained independence from British colonial rule on August 15, 1947. The Indian independence movement had been ongoing for several decades prior to that, led by figures such as Mahatma Gandhi, Jawaharlal Nehru, and Subhas Chandra Bose.\n",
      "\n",
      "The Indian independence movement was sparked by the partition of Bengal in 1905, which was a colonial-era administrative decision that divided the province into two parts: British India (West Bengal) and East Bengal. This led to widespread protests and resistance among Indians, who felt that their culture and identity were being eroded by British rule.\n",
      "\n",
      "The movement gained momentum in the early 20th century with the rise of Indian nationalist leaders such as Gandhi, Nehru, and Bose. Gandhi's non-violent resistance campaign, which included civil disobedience and boycotts, was particularly influential in mobilizing popular support for independence.\n",
      "\n",
      "The British government eventually agreed to grant independence to India through a series of negotiations and agreements. The Indian Independence Act was passed by the British Parliament on July 18, 1947, dividing British India into two separate countries: India and Pakistan (which later split into two parts: West Pakistan and East Pakistan).\n",
      "\n",
      "India gained independence at midnight on August 15, 1947, when Lord Louis Mountbatten, the last Viceroy of British India, handed over power to Jawaharlal Nehru, who became the first Prime Minister of independent India. The event was marked by celebrations across the country, and it marked a significant milestone in Indian history.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader=PyPDFLoader('/content/my_colab_env/SVM&KNN.pdf')\n",
    "documents=pdf_reader.load()\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "chunks=text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 118ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-421545398.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings=HuggingFaceEmbeddings()\n",
      "/tmp/ipython-input-421545398.py:1: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings=HuggingFaceEmbeddings()\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2313297066d43e193b5100cbd07e7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7c68c14dce41f18660daaa3543a490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c072bd690f4d49a025bc75d39c1e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d919f4a83e6847fa81aa7233bbf33825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02ac241b7dd425189bb3f37b15b9e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174d00edb0994d75a9d477835f14a0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8d3fd84cea453787d3178f4781936f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811689e37e4a446f973989d2fbc95264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6c2fbd0c774ca0b206f6dda58fe952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8b61c575914578980301ae8153cc26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9926d1e51ccc408183143fc6035499dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings=HuggingFaceEmbeddings()\n",
    "df=FAISS.from_documents(documents=chunks,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\")\n",
    "\n",
    "\n",
    "qa=ConversationalRetrievalChain.from_llm(llm=llm,retriever=df.as_retriever(),condense_question_prompt=CONDENSE_QUESTION_PROMPT,return_source_documents=True,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2457012892.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result=qa({\"question\":query,\"chat_history\":chat_history})\n"
     ]
    }
   ],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"Summarize the chapter\"\"\"\n",
    "result=qa({\"question\":query,\"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chapter discusses the concept of analogy and its importance in thinking, learning, and problem-solving. It explains how analogy is not just a tool for making connections between seemingly unrelated things, but rather it's a fundamental process that underlies all intelligent behavior. The author provides examples from history, science, and personal experience to illustrate how analogy has led to breakthroughs and insights in various fields. The chapter also highlights the limitations of traditional machine learning approaches that require a teacher or labeled data, and suggests that analogy can be used to fill this gap by allowing learners to generalize from one problem to another through similarities.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chapter discusses the power of analogy in thinking, learning, and problem-solving. It highlights how analogy can help us understand complex concepts by finding similarities between seemingly unrelated things. The authors present various examples, including historical events like the Cuban Missile Crisis and scientific discoveries like Darwin's theory of natural selection and Einstein's theory of relativity.\n",
      "\n",
      "The chapter also explores how analogy is used in machine learning, particularly in cases where there is limited data or no teacher to provide the right answer. It notes that even geniuses like Einstein rely on analogies to make connections and have breakthroughs.\n",
      "\n",
      "Overall, the chapter presents analogy as a fundamental aspect of human thinking and problem-solving, arguing that it is essential for learning and discovery.\n",
      "Ask the question: what is knn\n",
      "According to the given context, K-Nearest Neighbor (KNN) refers to an algorithm where a test example is classified by finding its k nearest neighbors and letting them vote. If the majority of the k nearest neighbors agree on a class, that class is assigned to the test example.\n",
      "Ask the question: what are the strengths of knn\n",
      "According to the text, the key advantages or strengths of K-Nearest Neighbor (KNN) algorithm are:\n",
      "\n",
      "* It can form a local model rather than a global one, which is useful for complex phenomena that are locally linear but not globally linear.\n",
      "* It's more robust than Nearest-Neighbor because it only goes wrong if a majority of the k nearest neighbors is noisy.\n",
      "* The bigger the database, the better it performs, making it suitable for large datasets.\n",
      "* It's computationally efficient at query time, unlike some other algorithms that may require expensive computations.\n",
      "\n",
      "Overall, KNN seems to be a simple yet powerful algorithm that can handle complex problems and large datasets with ease.\n",
      "Ask the question: what are the strengths of svm\n",
      "According to the provided context, the strengths of Support Vector Machines (SVM) include:\n",
      "\n",
      "* Resistance to overfitting, even in very high dimensions, due to margin maximization\n",
      "* The fewer support vectors an SVM selects, the better it generalizes\n",
      "* Expected error rate is at most the fraction of examples that are support vectors\n",
      "* More resistant to the curse of dimensionality than most\n",
      "* Can represent complex decision boundaries with a single optimum instead of multiple local optima\n",
      "\n",
      "These strengths contribute to SVMs' ability to achieve high accuracy and generalize well, as seen in their application to handwritten digit recognition.\n",
      "Ask the question: compare the svm and knn strengths\n",
      "Based on the provided context, here are the key differences and similarities between K-Nearest Neighbor (KNN) and Support Vector Machines (SVM):\n",
      "\n",
      "**Similarities:**\n",
      "\n",
      "* Both KNN and SVM aim to classify data points by finding a decision boundary or frontier.\n",
      "* They both rely on weighted voting or averaging of neighboring instances to make predictions.\n",
      "\n",
      "**Differences:**\n",
      "\n",
      "* **Accuracy:** SVMs are capable of setting new accuracy records, whereas KNN's accuracy is prone to overfitting, especially when the number of nearest neighbors (k) is high.\n",
      "* **Complexity:** SVMs have a single optimum for their weights, making learning easier and more reliable. In contrast, KNN has multiple local optima, which can lead to slower convergence or stuck in local minima.\n",
      "* **Expressiveness:** While both models are expressive, SVMs can learn smooth frontiers with sudden corners, whereas KNN's approximation is often less ideal due to its reliance on nearest neighbors.\n",
      "\n",
      "**Performance Influences:**\n",
      "\n",
      "* **Scenario 1 (Simple classification):** In this scenario, KNN might perform better than SVM since it is more robust and less prone to overfitting.\n",
      "* **Scenario 2 (Complex classification):** For complex classification tasks, SVMs are likely to outperform KNN due to their ability to learn smooth frontiers and handle noisy data.\n",
      "\n",
      "Please note that these differences and similarities are based on the provided context and might not be exhaustive.\n"
     ]
    }
   ],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"Summarize the chapter\"\"\"\n",
    "result=qa({\"question\":query,\"chat_history\":chat_history})\n",
    "print(result['answer'])\n",
    "chat_history.append((\"User\",query))\n",
    "chat_history.append((\"Assistant\",result['answer']))\n",
    "query=input(\"Ask the question: \")\n",
    "result=qa({\"question\":query,\"chat_history\":chat_history})\n",
    "print(result['answer'])\n",
    "for i in range(3):\n",
    "  chat_history.append((\"User\",query))\n",
    "  chat_history.append((\"Assistant\",result['answer']))\n",
    "  query=input(\"Ask the question: \")\n",
    "  result=qa({\"question\":query,\"chat_history\":chat_history})\n",
    "  print(result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"Explain KNN\"\"\"\n",
    "result=qa({\"question\":query,\"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here's an explanation of K-Nearest Neighbor (KNN):\n",
      "\n",
      "KNN is a type of nearest-neighbor algorithm where a test example is classified by finding its k nearest neighbors and letting them vote. If the majority of the k nearest neighbors agree on a class, that becomes the classification for the new test example.\n",
      "\n",
      "The key aspect of KNN is the value of k, which determines how many nearest neighbors are considered in the voting process. A higher value of k can lead to more robust predictions, as it's less likely that all k nearest neighbors will be noisy or misclassified. However, a higher k also means that the algorithm may lose some fine details and become less accurate.\n",
      "\n",
      "In summary, KNN is an extension of the nearest-neighbor algorithm that uses a voting process among k nearest neighbors to classify new examples.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"Explain SVM\"\"\"\n",
    "result=qa({\"question\":query,\"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVMs (Support Vector Machines) are a type of machine learning algorithm that was developed by Vladimir Vapnik. The core idea behind SVMs is to find the best separating hyperplane between two classes of data, while maximizing the margin or distance between the hyperplane and the nearest training examples.\n",
      "\n",
      "In simple terms, an SVM is like a map-guided mine detector. Imagine you're trying to walk across a no-man's land with mines buried underneath. You have a map that shows where the mines are located. Instead of taking any old path, you would take the safest route possible, which means giving the mines as much distance as possible. This is similar to how an SVM works. It learns to separate two classes by finding the border that maximizes the margin between them.\n",
      "\n",
      "SVMs are known for their ability to resist overfitting even in high-dimensional spaces, and they can achieve this by selecting only a few support vectors from the training data. These support vectors are the examples that are closest to the decision boundary and have the most influence on its shape.\n",
      "\n",
      "One of the key insights behind SVMs is that not all borders are created equal. A good border should be one that has a large margin or distance between it and the nearest training examples. This is because a larger margin means that the algorithm will be more robust to new, unseen data.\n",
      "\n",
      "SVMs also have some surprising properties. For example, even though they can form very complex decision boundaries in high-dimensional spaces, the actual boundary itself is always a straight line or hyperplane in a higher-dimensional space. This might seem counterintuitive at first, but it's actually a consequence of the mathematical formulation of the algorithm.\n",
      "\n",
      "Overall, SVMs are a powerful machine learning algorithm that has been widely used in many applications, including text classification, image recognition, and bioinformatics.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"List out the difference between KNN and SVM\"\"\"\n",
    "result=qa({\"question\":query,\"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here are the differences between K-Nearest Neighbor (KNN) and Support Vector Machines (SVM):\n",
      "\n",
      "1. **Border smoothness**: SVMs can learn smooth frontiers, whereas KNN's approximation is probably not ideal due to its jagged line with sudden corners.\n",
      "\n",
      "2. **Dimensionality**: SVMs can map data to a higher-dimensional space and find a maximum-margin hyperplane in that space, which may have infinite dimensions. In contrast, KNN operates in the original data space without dimensionality transformation.\n",
      "\n",
      "3. **Kernel choice**: SVMs require choosing the kernel (similarity measure) beforehand, whereas KNN uses a fixed distance metric.\n",
      "\n",
      "4. **Margin of safety**: SVMs learn a border with a margin of safety around each example, which allows for greater flexibility in separating positive and negative classes. KNN does not have this concept.\n",
      "\n",
      "5. **Learning process**: SVMs choose support vectors and their weights to separate classes, whereas KNN relies on the weighted sum of neighboring examples.\n",
      "\n",
      "Please note that these differences are based on the provided context and might not be an exhaustive list of all differences between KNN and SVM.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"What are the assumptions of SVM\"\"\"\n",
    "result=qa({\"question\":query,\"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the assumptions of SVMs (Support Vector Machines) include:\n",
      "\n",
      "* The data lives in a higher-dimensional space than initially thought.\n",
      "* By mapping the data to this higher-dimensional space and finding a maximum-margin hyperplane, SVMs can resist overfitting even in high dimensions.\n",
      "* The kernel used in SVMs allows for the mapping of data to a higher-dimensional space, which may have infinite dimensions.\n",
      "* The support vectors play a crucial role in defining the frontier between positive and negative classes.\n",
      "* The margin maximization aspect of SVMs helps prevent overfitting.\n",
      "\n",
      "Please note that these assumptions are based on the provided context and might not be an exhaustive list of all assumptions underlying SVMs.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"What are the assumptions of KNN\"\"\"\n",
    "result=qa({\"question\":query,\"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The assumptions of KNN (K-Nearest Neighbor) are:\n",
      "\n",
      "* Each data point is its own little classifier, predicting the class for all query examples it wins.\n",
      "* The algorithm relies on the principle that similar instances should be classified similarly.\n",
      "\n",
      "In other words, the main assumption of KNN is that the majority vote of the k nearest neighbors will correctly classify a new instance.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 117ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://535b8259c439f2b899.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://535b8259c439f2b899.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## My code\n",
    "def chat_fn(message, history):\n",
    "    \"\"\"\n",
    "    message: latest user input (string)\n",
    "    history: list of [user_msg, assistant_msg] pairs\n",
    "    \"\"\"\n",
    "    # Convert Gradio's history format into LangChain format\n",
    "    chat_history = []\n",
    "    for human, ai in history:\n",
    "        chat_history.append((\"User\", human))\n",
    "        chat_history.append((\"Assistant\", ai))\n",
    "\n",
    "    # Run LangChain ConversationalRetrievalChain\n",
    "    result = qa({\"question\": message, \"chat_history\": chat_history})\n",
    "    answer = result[\"answer\"]\n",
    "\n",
    "    # Return only the answer (Gradio handles updating history)\n",
    "    return answer\n",
    "\n",
    "chatbot = gr.ChatInterface(\n",
    "    fn=chat_fn,\n",
    "    title=\"Student's Companion\",\n",
    "    description=\"Ask you questions regarding the paper uploaded- SVM and KNN\",\n",
    "    examples=[\"Summarize the chapter\", \"Explain SVM\"]\n",
    ")\n",
    "\n",
    "chatbot.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import gradio as gr\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize the Ollama model\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Define your system prompt\n",
    "system_message = \"You are a helpful assistant.\"\n",
    "\n",
    "def chat(message, history):\n",
    "    # Add system message at the beginning\n",
    "    messages = [SystemMessage(content=system_message)]\n",
    "\n",
    "    # Convert history into LangChain message objects\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            messages.append(AIMessage(content=msg[\"content\"]))\n",
    "\n",
    "    # Add the new user message\n",
    "    #message = add_context(message)\n",
    "    messages.append(HumanMessage(content=message))\n",
    "\n",
    "    # Stream the response from the model\n",
    "    response = \"\"\n",
    "    for chunk in llm.stream(messages):\n",
    "        response += chunk.content\n",
    "        yield response\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
